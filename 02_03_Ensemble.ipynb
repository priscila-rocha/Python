{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comitês de máquinas (Ensemble)\n",
    "\n",
    "Os comitês de máquinas são a combinação de um mais classificadores / regressores para chegar a um rótulo. Nesse sentido, podem ser empregados classificadores que foram treinados com diferentes conjuntos de dados, ou poderão ser empregados classificadores que foram treinados no mesmo conjunto de dados utilizando estratégias de aprendizado diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Acurácia de KNeighbors simples:', scores.mean())\n",
    "\n",
    "model = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5, random_state = 42)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Acurácia de KNeighbors Bagging (c/ 10 estimators):', scores.mean())\n",
    "\n",
    "model = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5, n_estimators=100, random_state = 42)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Acurácia de KNeighbors Bagging (c/ 100 estimators):', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Acurácia de Decision Tree puro:', scores.mean())\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Acurácia de Random Forest:', scores.mean())\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print('Acurácia de Extreme Randomized Trees:', scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb',clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"Acurácia: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, eclf], ['Logistic Regression', 'Random Forest', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"Acurácia: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.3, max_depth=2, random_state=0)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print('Acurácia de Gradient Boosting Tree:', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "\n",
    "(1) Em um novo notebook, avaliar a combinação das técnicas de seleção de atributos e redução de dimensionalidade com as métricas de desempenho apropriadas e verificar se os comitês de máquina (ensemble) auxiliam na melhora do resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Dica: Tire as medidas de tempo para conhecer a relação de custo e desempenho dos modelos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
